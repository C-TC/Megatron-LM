{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong scaling\n",
    "MILP solver timeout after 200s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_schedule import Simulator, AutoScheduleStore, AutoScheduleResult, get_best_BFSPP_schedule\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def is_milp_optimal(sim: AutoScheduleResult):\n",
    "    if not sim:\n",
    "        return False\n",
    "    if sim.lp_status is not None and sim.lp_status == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def plot_weak_scaling_runtime(n_DC, num_chunk):\n",
    "    seq_len = 8192\n",
    "    tp=8\n",
    "    num_mb_per_pp_stage = 4\n",
    "    dp = 64\n",
    "    store = AutoScheduleStore(\"405_ud_store.pkl\", \"405_wave_store.pkl\")\n",
    "    n_layer_l = [32, 64, 128]\n",
    "    n_pp_l = [2, 4, 8]\n",
    "    runtime = []\n",
    "    optimal_runtime = []\n",
    "    tokens_per_iter_per_gpu = []\n",
    "    bfs_tokens_per_sec_per_gpu = []\n",
    "    is_milp_optimal_l = []\n",
    "    \n",
    "    for num_layers, pp in zip(n_layer_l, n_pp_l):\n",
    "        \n",
    "        bfs_sim = get_best_BFSPP_schedule(\n",
    "            llama_model_size=405,\n",
    "            seq_len=seq_len,\n",
    "            mbs=1,\n",
    "            tp=tp,\n",
    "            pp=pp,\n",
    "            dp=dp,\n",
    "            gpu_mem_bytes=88 * 1024**3,  # around 96GB\n",
    "            gpu_avg_perf_flops=350 * 10**12,\n",
    "            num_DC=n_DC,\n",
    "            DC_comm_latency=0.01,\n",
    "            DC_comm_bandwidth=32 * 10**9,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "        bfs_tokens_per_sec_per_gpu.append(bfs_sim.get_tokens_per_second_per_device())\n",
    "        \n",
    "        if pp % n_DC != 0 or num_layers % (pp * 2) != 0:\n",
    "            runtime.append(None)\n",
    "            optimal_runtime.append(None)\n",
    "            tokens_per_iter_per_gpu.append(None)\n",
    "            is_milp_optimal_l.append(None)\n",
    "            continue\n",
    "        \n",
    "        sim = Simulator(\n",
    "            llama_model_size=405,\n",
    "            seq_len=seq_len,\n",
    "            mbs=1,\n",
    "            tp=tp,\n",
    "            pp=pp,\n",
    "            dp=dp,\n",
    "            num_mb_per_pp_stage=num_mb_per_pp_stage,\n",
    "            num_chunks=num_chunk,\n",
    "            gpu_mem_bytes=88 * 1024**3,  # around 96GB\n",
    "            gpu_avg_perf_flops=350 * 10**12,\n",
    "            num_DC=n_DC,\n",
    "            DC_comm_latency=0.01,\n",
    "            DC_comm_bandwidth=32 * 10**9,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        tokens_per_iter_per_gpu.append(sim.get_tokens_per_iteration() / sim.pp / sim.dp / sim.tp)\n",
    "        if num_chunk == 1:\n",
    "            sim_res = store.get_ud_schedule_result(\n",
    "                sim.get_system_config(), compute_if_not_exist=False\n",
    "            )\n",
    "        else:\n",
    "            sim_res = store.get_wave_schedule_result(\n",
    "                sim.get_system_config(), compute_if_not_exist=False\n",
    "            )\n",
    "        if not sim_res or sim_res.objective_value is None:\n",
    "            runtime.append(None)\n",
    "            optimal_runtime.append(sim.get_optimal_runtime())\n",
    "            is_milp_optimal_l.append(False)\n",
    "        else:    \n",
    "            runtime.append(sim_res.objective_value / sim.time_scale_factor)\n",
    "            optimal_runtime.append(sim.get_optimal_runtime())\n",
    "            is_milp_optimal_l.append(is_milp_optimal(sim_res))\n",
    "    \n",
    "    normalized_runtime = [runtime[i] / optimal_runtime[i] if runtime[i] is not None else None for i in range(len(runtime))]\n",
    "    ones = [1 for _ in range(len(n_layer_l))]\n",
    " \n",
    "    tok_per_sec_per_gpu = [tokens_per_iter_per_gpu[i] / runtime[i] if runtime[i] is not None else None for i in range(len(runtime))]\n",
    "    opt_tok_per_sec_per_gpu = [tokens_per_iter_per_gpu[i] / optimal_runtime[i] if optimal_runtime[i] is not None else None for i in range(len(runtime))]\n",
    "    # plot\n",
    "    # subplots\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 4))\n",
    "    axes[0].plot(n_layer_l, opt_tok_per_sec_per_gpu, label=\"Optimal\", marker='o')\n",
    "    axes[0].plot(n_layer_l, tok_per_sec_per_gpu, label=\"Unidirectional\", marker='^')\n",
    "    axes[0].plot(n_layer_l, bfs_tokens_per_sec_per_gpu, label=\"BFS\", marker='s')\n",
    "    for layer, tok, optimal in zip(n_layer_l, tok_per_sec_per_gpu, is_milp_optimal_l):\n",
    "        if not optimal:\n",
    "            axes[0].scatter(layer, tok, color='r', marker='x', s=100)\n",
    "    \n",
    "    axes[1].plot(n_layer_l, ones, label=\"Optimal\", marker='o')\n",
    "    axes[1].plot(n_layer_l, normalized_runtime, label=\"Unidirectional\", marker='^')\n",
    "    \n",
    "    axes[0].set_xticks(n_layer_l)\n",
    "    axes[1].set_xticks(n_layer_l)\n",
    "    axes[0].set_xlabel(\"Number of layers\")\n",
    "    axes[0].set_ylabel(\"Tokens/sec/GPU\")\n",
    "    axes[1].set_xlabel(\"Number of layers\")\n",
    "    axes[1].set_ylabel(\"Normalized Runtime (s)\")\n",
    "    schedule = \"wave\" if num_chunk > 1 else \"ud\"\n",
    "    fig.suptitle(f'Weak scaling: l/pp = 16, {schedule} schedule, {n_DC} DCs')\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_weak_scaling_runtime(2,1)\n",
    "plot_weak_scaling_runtime(4,1)\n",
    "plot_weak_scaling_runtime(2,2)\n",
    "plot_weak_scaling_runtime(4,2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandwidth Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_bw_scaling_runtime(n_DC, num_chunk):\n",
    "    seq_len = 8192\n",
    "    tp=8\n",
    "    num_layers = 128\n",
    "    num_mb_per_pp_stage = 4\n",
    "    dp = 64\n",
    "    pp = 4\n",
    "    store = AutoScheduleStore(\"405_ud_store.pkl\", \"405_wave_store.pkl\")\n",
    "    DC_bw_l = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    runtime = []\n",
    "    optimal_runtime = []\n",
    "    tokens_per_iter_per_gpu = []\n",
    "    bfs_tokens_per_sec_per_gpu = []\n",
    "    is_milp_optimal_l = []\n",
    "    \n",
    "    for DC_bw in DC_bw_l:\n",
    "        DC_comm_bandwidth = DC_bw * 10**9\n",
    "        \n",
    "        bfs_sim = get_best_BFSPP_schedule(\n",
    "            llama_model_size=405,\n",
    "            seq_len=seq_len,\n",
    "            mbs=1,\n",
    "            tp=tp,\n",
    "            pp=pp,\n",
    "            dp=dp,\n",
    "            gpu_mem_bytes=88 * 1024**3,  # around 96GB\n",
    "            gpu_avg_perf_flops=350 * 10**12,\n",
    "            num_DC=n_DC,\n",
    "            DC_comm_latency=0.01,\n",
    "            DC_comm_bandwidth=DC_comm_bandwidth,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "        bfs_tokens_per_sec_per_gpu.append(bfs_sim.get_tokens_per_second_per_device())\n",
    "        \n",
    "        if pp % n_DC != 0 or num_layers % (pp * 2) != 0:\n",
    "            runtime.append(None)\n",
    "            optimal_runtime.append(None)\n",
    "            tokens_per_iter_per_gpu.append(None)\n",
    "            is_milp_optimal_l.append(None)\n",
    "            continue\n",
    "        \n",
    "        sim = Simulator(\n",
    "            llama_model_size=405,\n",
    "            seq_len=seq_len,\n",
    "            mbs=1,\n",
    "            tp=tp,\n",
    "            pp=pp,\n",
    "            dp=dp,\n",
    "            num_mb_per_pp_stage=num_mb_per_pp_stage,\n",
    "            num_chunks=num_chunk,\n",
    "            gpu_mem_bytes=88 * 1024**3,  # around 96GB\n",
    "            gpu_avg_perf_flops=350 * 10**12,\n",
    "            num_DC=n_DC,\n",
    "            DC_comm_latency=0.01,\n",
    "            DC_comm_bandwidth=DC_comm_bandwidth,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        tokens_per_iter_per_gpu.append(sim.get_tokens_per_iteration() / sim.pp / sim.dp / sim.tp)\n",
    "        if num_chunk == 1:\n",
    "            sim_res = store.get_ud_schedule_result(\n",
    "                sim.get_system_config(), compute_if_not_exist=False\n",
    "            )\n",
    "        else:\n",
    "            sim_res = store.get_wave_schedule_result(\n",
    "                sim.get_system_config(), compute_if_not_exist=False\n",
    "            )\n",
    "        if not sim_res or sim_res.objective_value is None:\n",
    "            runtime.append(None)\n",
    "            optimal_runtime.append(sim.get_optimal_runtime())\n",
    "            is_milp_optimal_l.append(False)\n",
    "        else:    \n",
    "            runtime.append(sim_res.objective_value / sim.time_scale_factor)\n",
    "            optimal_runtime.append(sim.get_optimal_runtime())\n",
    "            is_milp_optimal_l.append(is_milp_optimal(sim_res))\n",
    "    \n",
    "    normalized_runtime = [runtime[i] / optimal_runtime[i] if runtime[i] is not None else None for i in range(len(runtime))]\n",
    "    ones = [1 for _ in range(len(DC_bw_l))]\n",
    "    tok_per_sec_per_gpu = [tokens_per_iter_per_gpu[i] / runtime[i] if runtime[i] is not None else None for i in range(len(runtime))]\n",
    "    opt_tok_per_sec_per_gpu = [tokens_per_iter_per_gpu[i] / optimal_runtime[i] if optimal_runtime[i] is not None else None for i in range(len(runtime))]\n",
    "    # plot\n",
    "    # subplots\n",
    "    schedule = \"Wave\" if num_chunk > 1 else \"Unidirectional\"\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12, 4))\n",
    "    axes[0].plot(DC_bw_l, opt_tok_per_sec_per_gpu, label=\"Optimal\", marker='o')\n",
    "    axes[0].plot(DC_bw_l, tok_per_sec_per_gpu, label=schedule, marker='^')\n",
    "    axes[0].plot(DC_bw_l, bfs_tokens_per_sec_per_gpu, label=\"BFS\", marker='s')\n",
    "    for layer, tok, optimal in zip(DC_bw_l, tok_per_sec_per_gpu, is_milp_optimal_l):\n",
    "        if not optimal:\n",
    "            axes[0].scatter(layer, tok, color='r', marker='x', s=100)\n",
    "    \n",
    "    axes[1].plot(DC_bw_l, ones, label=\"Optimal\", marker='o')\n",
    "    axes[1].plot(DC_bw_l, normalized_runtime, label=\"Unidirectional\", marker='^')\n",
    "    \n",
    "    axes[0].set_xticks(DC_bw_l)\n",
    "    axes[0].set_xlabel(\"Inter DC Bandwidth (GB/s)\")\n",
    "    axes[0].set_xscale('log', base=2)\n",
    "    axes[1].set_xticks(DC_bw_l)\n",
    "    axes[1].set_xlabel(\"Inter DC Bandwidth (GB/s)\")\n",
    "    axes[1].set_xscale('log', base=2)\n",
    "    \n",
    "    axes[0].set_ylabel(\"Tokens/sec/GPU\")\n",
    "    axes[1].set_ylabel(\"Normalized Runtime (s)\")\n",
    "    fig.suptitle(f'Bandwidth scaling: {schedule} schedule, {n_DC} DCs')\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_bw_scaling_runtime(2,1)\n",
    "plot_bw_scaling_runtime(4,1)\n",
    "plot_bw_scaling_runtime(2,2)\n",
    "plot_bw_scaling_runtime(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
